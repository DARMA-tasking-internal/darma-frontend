
\section{Compatible Memory Models}
<<<<<<< HEAD
\label{sec:memmodel}
\todo[inline]{Janine: Change uses of handle throughout doc to data handle.  For
  now I kept terminology as handle keep glossary links working correctly}

\gls{DARMA} implements a \gls{distributed memory model}, with each \gls{rank}
in \gls{DARMA} conceptually having its own private memory space. 
Inter-\gls{rank} communication occurs indirectly via a 
\gls{key-value store} or \gls{tuple space} abstraction using
\gls{coordination semantics} (such as \codelink{publish} and
\codelink{fetch}). 

In \gls{DARMA} a data \gls{handle} is conceptually a \gls{reference counted pointer} into the
\gls{key-value store}.  Data \glspl{handle} are used to manage the
complexities associated with \gls{task parallelism} and inter-\gls{rank} communication.  
When data needs to be made accessible off-\gls{rank}, the application developer 
\codelink{publish}es the \gls{handle}.  Each \gls{handle} has a globally unique handle ID
(e.g., a \codelink{key} into the \gls{key-value store}).  
In addition to facilitating communication, \gls{handle} data structures track
additional information to support \gls{sequential semantics} (see Chapter~\ref{chap:translation_layer} for details on how this is supported).

The \gls{declarative} programming style of \gls{DARMA} is provided in part by \gls{coordination
semantics}:  rather than explicitly move data between \glspl{rank} via direct communication
(i.e.,  \inlinecode{send/recv}), processes \emph{coordinate} by
publishing/fetching data associated with unique \codelink{key}s in a
\gls{key-value store}.
\Gls{coordination semantics} promote out-of-order message arrival,
\gls{deferred execution}, and \gls{migratable} work 
since the application declares or describes the data it needs/produces rather than enforcing an explicit delivery mechanism.
In addition to \gls{coordination semantics}, \gls{asynchronous} collectives
between \glspl{handle} are specified. 


\todo[inline]{Jeremy: I would like to add a few sentences here regarding how
  this is a generalization of pinned vs unpinned memory, and that this is in
    fact a part of the reason for the distributed memory with coordination
    semantics design decision}

\todo[inline]{David/Jeremy: We've had talks about the potential for supporting
  the equivalent of distributed shared memory + coordiantion semantics - can
    we say anything speculative about that here? or at least say why we are not
    supporting that in this version of the specification?}
=======
\label{sec:mem_model}

The memory model for DARMA encompasses how variables are accessed (pointer, iterator, accessor) and when updates become visible to parallel threads (concurrency).  
DARMA provides a global memory space in which data are identified by unique tuple identifiers.  
Any objects published into the tuple space can be read/written by any thread/process. 
This is similar to UPC which provides a global address space, but data is always accessed by pointer address rather than tuple. 
This further contrasts with MPI 2-sided in which data are accessed by address, but there is no global memory space.  
Data within in an MPI process is ?private? and can only be exchanged via messages.  
DARMA , like UPC, further contrasts with PGAS (partitioned global address spaces) in that the global memory space is ?flat? and is not explicitly partitioned across processes.
The tuple space vs address space is only relevant for creating and managing coarse-grained tasks. 
Within a task, tuple identifiers are resolved to a specific local address and the standard C++ memory model applies.

Also critical is concurrency in the memory model and when/how updates data are made visible to parallel threads.  
Again, within tasks, the C++ memory model applies.  
At the task level (coarse-grained), DARMA ensures atomicity of all tasks. 
The DARMA scheduler enforces the C++ sequential consistency model at the level of tasks in the same way that C++ ensures sequential consistency at the level of instructions. 
DARMA understands read/write usages of tasks and ensures that writes are always visible to subsequent reads - and reads always complete before subsequent writes.  
This happens automatically within a process (shared memory), but requires an extra annotation by the app developer for objects exchanged between processes (distributed memory) to ensure data consistency.

The details of the global memory model are managed through a data \glspl{handle} object.  
The application developer \inlinecode{publish}es data associated with a handle that should be globally visible
outside of its \gls{rank}.  
When publishing, the user must specify an \gls{access group} for that data.  
Declaring an access group informs the runtime who currently needs or will need the data,  
allowing garbage collection and \gls{anti-dependency} resolution.
In most cases, the access group will be declared as the number of readers (1, in the case of simple point-to-point send).
Once all read handles are released (go out of scope in C++ terms), garbage collection or anti-dependency resolution can occur.

>>>>>>> darma/0.3-devel



