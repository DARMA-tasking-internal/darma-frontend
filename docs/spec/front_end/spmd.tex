%!TEX root = ../sandReportSpec.tex

\section{SPMD support}
\label{sec:spmd}
Most applications written in or ported to \gls{DARMA} will likely have \gls{spmd} as
the dominant form of parallelism.
To simplify the implementation of \gls{spmd}-structured codes, the notion of 
a \gls{rank} is maintained within the \gls{API}.   
Again, rather than rely entirely on \gls{sequential semantics} in cases of massive data parallelism,
many independent parallel \glspl{execution stream} can begin simultaneously and
coordinate via the \gls{key-value store}.
Each \gls{execution stream} is assigned a unique \gls{rank} ID, analogous to the MPI rank assigned to processes in a MPI communicator.
The initialization and termination of the runtime in each \gls{execution
stream} is via the calls \codelink{darma_init} and \codelink{darma_finalize}. 
The total number of \gls{spmd} \glspl{execution stream} are queried with the call \codelink{darma_spmd_size},
and the \gls{rank} ID of a particular \gls{exectuion stream{ is queried with
    \codelink{darma_spmd_rank}. A typical user written main program will look as follows:
\begin{CppCode}
int darma_main(int argc, char**argv){
  darma_init(argc, argv);
  size_t n_ranks = darma_spmd_size();
  size_t me = darma_spmd_rank();
  ...
  darma_finalize();
  return 0;
}
\end{CppCode}

The \gls{rank} is a very useful concept to orchestrate dependencies in a \gls{spmd}
model since data pertaining to a \gls{rank} can be associated with \codelink{key}s that utilize
the \gls{rank} for uniqueness. The example below illustrates this concept, where
the \gls{rank} is integral to the \codelink{key} associated with data
originating on that \gls{rank}.
\begin{CppCode}
size_t me = darma_spmd_rank();
auto data_handle = initial_access<double>("data_key", me);
\end{CppCode}  
Note that in \gls{DARMA}, \gls{spmd} \glspl{rank} are actually just a special
kind of \gls{task} that happens to have a name containing the \gls{rank}, and can be treated as such. 
\todo{Jeremy: Does this sentence about ranks being a special kind of task still make sense given the execution stream
additions?}
However, the similarity to traditional, MPI-style \gls{spmd} upon launch should improve the ease of porting and scalability significantly.

We emphasize again that within \gls{DARMA}'s supprot for \gls{spmd}, 
coordinating (rather than communicating) abstracts physical data locations to better support task migration.
Additionally, it removes message-ordering requirements to better support asynchronous data transfers.
We further reiterate that even though the \gls{key-value store} appears to the
application developer to be a traditional data store, it can be implemented in a scalable distributed fashion.

