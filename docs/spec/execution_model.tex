%!TEX root = sandReportSpec.tex


\section{Execution Models}
\label{sec:exec_model}
The main focus of \gls{DARMA} is the \gls{programming model} and corresponding
\gls{translation layer} that maps a program expressed via a combination of
\gls{CSP} semantics, \gls{coordination semantics}, and additional \CC{}-embedded task annotations into a generic data-flow based
description of an algorithm based on deferred tasks.

\gls{DARMA}-compliant \gls{back end} \gls{runtime systems} are required to enable an efficient
\gls{spmd} launch of their program, similar to an MPI launch.
This is based off application developer feedback, which has indicated that 
two of the most critical challenges for scientific applications with massive data parallelism in a task-based model
include initial problem decomposition and distribution.
\gls{DARMA}'s efficient \gls{spmd} runtime-based launch requirement 
will be modified if solutions are developed to support massive \gls{spmd} launches
through compiler-based transformations.

Other than this requirement, \gls{DARMA} prescribes very little
about execution.
For example, \gls{DARMA} prescribes nothing about the scheduling of tasks nor the implementation 
of the data structures (e.g., \gls{key-value store}, \gls{tuple space})
  required to support \gls{coordination semantics}.
A \gls{back end} \gls{runtime system} scheduler is therefore free to use, for example,
either depth-first or breadth-first priorities in deferred tasks (as captured
in a \gls{CDAG}).
Similarly, a scheduler may use \glspl{thread pool} with work queues to manage
tasks or it may use a \gls{fork-join} model that creates new threads for each task.
In this way, \gls{DARMA} codes are \gls{execution model}-agnostic, only
requiring that a \gls{back end} \gls{runtime system} preserve the
\glspl{data-flow dependency} expressed in the application and derived by the
\gls{translation layer}.

The main focus of DARMA is the programming model and corresponding translation layer that maps CSP semantics onto a data-flow description of the algorithm based on deferred tasks.
DARMA prescribes nothing about the scheduling of tasks or implementation of key-value store semantics supporting process coordination (see below).
A backend scheduler is therefore free to use, e.g. either depth-first or breadth-first priorities in executing tasks on the CDAG.
Similarly, a scheduler may use thread pools with work queues to run tasks or it may use a fork-join model that creates new threads for each task.
DARMA codes are therefore execution model-agnostic, only requiring that a backend runtime preserve the data-flow constraints expressed in the application and derived by the translation layer.

DARMA furthermore prescribes nothing about the internals of each task.
DARMA is fully compatible with parallel elastic tasks - tasks with flexible fine-grained parallelism, usually data parallelism.
For example, depending on dynamic conditions, more or fewer threads may be requested for a GPU kernel.
Although the DARMA application-level API currently only allows expressing task granularity and task data-flow,
we expect the API to also express task elasticity in future versions.

