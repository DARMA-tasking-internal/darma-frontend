%!TEX root = sandReportSpec.tex


\section{Programming Model}
\label{sec:programmingmodel}
\todo[inline]{add task to glossary}


\Glspl{programming model} provide application developers 
abstractions for expressing \emph{correct} and \emph{performant} algorithms. 
As described earlier, a key design goal of \gls{AMT} models is to enable performance-based
optimizations of code dynamically at runtime.
Runtime-based optimizations come with an associated runtime cost, which is what
motiviates the use of \glspl{task} (rather than, e.g., instructions) as the basis for dynamic runtime transformations.
Existing \gls{AMT} models provide a variety of \glspl{API} for capturing and expressing
\glspl{data-flow dependency} and communicating these to the underlying
\gls{runtime system}.  
One of the adoption challenges many of these \glspl{runtime system} face is
that they require a significant shift away from what has become
the defacto standard of distributed HPC programming: \gls{CSP}. 

\gls{DARMA}'s \gls{programming model} seeks to facilitate the expression of
deferred, asysnchronous work, enabling a \gls{back end} \gls{runtime system}
to perform dynamic runtime optimizations, while making it as simple as possible
for programmers to reason about the correctness of their code.
This motivates \gls{DARMA}'s combined use of successful \gls{programming model}
concepts from a variety of existing \glspl{runtime system}. 

One of \gls{DARMA}'s \gls{programming model} key design decisions is rooted in
the following observations: 
1) all application
developers can effectively reason about how to write correct sequential codes,
2) all MPI
programmers can effectively reason about how to write correct \gls{CSP} codes,
3) most applications written in or ported to \gls{DARMA} will likely
have \gls{spmd} as their dominant parallelism.
To simplify the implementation of \gls{spmd}-structured codes, the notion of a
\codelink{rank} is maintained within the \gls{API}.   
By maintaining the notion of a \codelink{rank}, \gls{DARMA} provides application developers a convenience mechanism for creating
the initial problem decomposition and distribution.
Immediately after launch, any user-specified \gls{deferred work} is free to be
migrated by the \gls{runtime system}, if it will result in
better performance.
Because \gls{DARMA} maintains the notion of a \codelink{rank}, it is also
possible for \gls{DARMA} to maintain \gls{CSP} semantics within an initial
implementation or port of a code, 
so that developers can express correctness constraints through a familiar and
intuitive \gls{programming model}.  
\gls{DARMA} facilitates the expression of hybrid parallelism by supporting  
\gls{sequential semantics}, within a \codelink{rank}.  This means that application
developers can reason about code as
though it were being deployed sequentially within the \codelink{rank}, even in
the presence of user-specified \gls{deferred work}.   

\gls{DARMA} employs \CC-embedded task annotations for the specification of \gls{deferred
work}. Each block of \gls{deferred work} can be considered a task (coarse-grained blocks of 
\gls{procedural} \gls{imperative} code),  which is not necessarily performed in program order.   Instead,
\gls{deferred work} is performed asynchronously when all of its \glspl{data-flow dependency} are
satisfied.  \Gls{task parallelism} is primarily achieved through permissions/access qualifiers
on data that enable that enable a runtime to reason about which tasks can run in parallel and which tasks are strictly ordered.
Task granularity is determined by the user and annotations are translated by \gls{DARMA}'s \gls{translation
layer} through standard \CC\ constructs (e.g., \glspl{lambda}, \glspl{reference counted pointer}) and \gls{template metaprogramming} to
expose \gls{task parallelism} inherent in the code.
%The issue of granularity is therefore of paramount importance in task-based model.
%How should the extra flexibility of fine-grained tasks be balanced against amortizing the cost of runtime analysis?
%For the time-being, the choice of a correct task granularity seems beyond the abilities of current compilers.
%The application-level API therefore expresses both correctness and granularity.
We note here that \gls{DARMA}'s runtime optimizations are complementary to
compile-time optimizations performed by \emph{performance
portability} tools, e.g.~cite[Kokkos,RAJA,TiDA].  Compile-time performance
portability tools provide the ability to map a
single code onto high-performance execution across diverse compute platforms.
\todo[inline]{Jeremy/David: Can/should we say more about interoperability with
perf portability layers here}


Communication between \gls{DARMA} ranks is not performed via direct messaging,
differentiating it different from a \gls{CSP} model.  Instead, \gls{coordination
semantics} are used:   
rather than explicitly move data between ranks via direct communication
(i.e.,  \inlinecode{send/recv}), processes \emph{coordinate} by putting/getting data associated with a unique \inlinecode{key} in a
\gls{key-value store} or \gls{tuple space}.  
\Gls{coordination semantics} promote out-of-order message arrival, deferred execution, and task migration
since the application declares or describes the data it needs/produces rather than enforcing an explicit delivery mechanism.
The key to providing performant coordination semantics in \gls{DARMA} is 
the use of \gls{zero-copy} mechanisms and tuple caching that 
produce execution equivalent to an MPI \inlinecode{send/recv} code. 
\todo[inline]{Jeremy, David: Are these mechanisms that are key to performance
  achieved in the translation layer? Or, are these requirements to runtime
teams in their implementation?}


Together, these features make \gls{DARMA} a mixed \gls{imperative}/\gls{declarative} \gls{programming model}.
As much as possible, sequential imperative semantics are used to produce intuitive, maintainable code.
However, the ``procedural imperative'' function calls and code blocks do not execute immediately.
Rather than explicitly perform all work in program order and block on data requests.
The ability to defer work and advance ahead is what gives the \gls{back end}
\gls{runtime  system} the ability to make performance-improving transformations.
\Gls{deferred execution} makes \gls{DARMA} \emph{declarative}, leaving the
exact control-flow up to the \gls{runtime system}.



%The application-level API therefore expresses both correctness and granularity.
%The issue of task granularity is therefore of paramount importance in a task-based model.
%How should the extra flexibility of fine-grained tasks be balanced against amortizing the cost of runtime analysis?
%For the time-being, the choice of a correct task granularity is beyond the abilities of current compilers.
%As such, choosing task granularity is the responsibility of the application developer.



%Correctness and performance portability must both balance the level of abstraction.
%Algorithms written at too low a level are highly error-prone and not portable.
%In particular, they may over-express correctness constraints.
%A compiler/runtime must obey the correctness requirements (e.g. instruction order) prescribed by an application code.
%Low-level code may be unnecessarily prevent the compiler or runtime from optimizing program execution.
%For example, code hard-wired for a particular loop structure and data layout may perform very well on one system,
%but that may prevent the compiler from tiling optimizations of the loops on other systems. 
%What is the correct way and level of abstraction for programmers to 1) express constraints (correctness) while 2) still enabling the compiler/runtime to transform execution (performance portability) while also 3) allowing developers to direct (hand-optimize) execution when the compiler/runtime misses optimizations apparent to a human developer. 


Although not yet supported in version \specVersion\ of the specification, several
important features will play a role in the \gls{DARMA} \gls{programming model}:
\todo[inline]{David/Jeremy: please vet/edit this list as appropriate}
\begin{compactdesc}
\item{\bf Expressive Underlying Abstract Machine Model:}
Notions of \glspl{execution space} and \glspl{memory space} will be introduced
formally in later
versions of the specification.  These abstractions (or similar ones) appear in other runtime
solutions, e.g. \cite{Kokkos, RAJA}, to
  address deficiencies in the \gls{abstract machine model} used by 
  \glspl{runtime system} that support \gls{spmd} parallelism (i.e., uniform compute elements, flat memory
    spaces).  Using such abstractions
1) facilitates performance portable application development across 
  a variety of \glspl{execution space}, and 2)
  provides finer-grained control and additional flexibility in the
  communication of policies regarding data locality and data movement. 
\item{\bf Runtime peformance introspection}
  In future versions of the specification \gls{DARMA} will specify hooks for the
  application developer to express, guide, and leverage the use of runtime-level
  performance \gls{introspection}. An important \gls{co-design} activity will include determining
  whether peformance \gls{introspection} needs to factor into the
  application-level \gls{programming
  model} on the \gls{front end} at all, or whether it purley belongs as part of the  \gls{back end}
  \gls{runtime system} \gls{API}.
\item{\bf Expression of fine-grained deferred parallel patterns}.
  In future versions of the specification, \gls{DARMA} will 
  specify deferred fine-grained parallel patterns, e.g., deferred
  \inlinecode{parallel-for}, \inlinecode{parallel-scan}, etc.
\item{\bf Field slicing in classes and class member functions as tasks}
  \todo[inline]{David/Jeremy: please elaborate here - can we say something here about
  the role we anticipate tasks as member functions to play.}
\item {\bf Data Staging:}
The memory and execution space concepts introduced above enable 1) performance portable tasks that can run in 
multiple environments through a single code and 2) user-directed placement hints to tell the runtime where tasks should run
\item {\bf Collectives:}
  Some collectives will be supported by \gls{DARMA} in version 0.3.1 of
  the specification, including \inlinecode{all-reduce},
  \inlinecode{reduce-scatter}, and \inlinecode{barrier} collectives.
Collectives will be data-centric rather rank-centric, as done in MPI.
\item{\bf Programmer-directed optimization}
While an abstract algorithm may make more information available to the compiler or runtime for performance-tuning transformations,
compilers and runtime schedulers may not always understand the global nature of the problem.
As such, they may not make peformance-improving optimizations that are apparent to an application developer.
A critically important part of future co-design activities will be teh
development of the interface by which developers can steer the runtime towards
a desired set of optimizations that compilers or runtime schedulers might fail
to perform.
\todo[inline]{Jeremy: I moved this here and rephrased a bit -- please vet}
\end{compactdesc}






 

