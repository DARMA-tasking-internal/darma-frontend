%!TEX root = sandReportSpec.tex


\section{Programming Model}
\label{sec:programmingmodel}

\Glspl{programming model} provide application developers 
abstractions for expressing \emph{correct} and \emph{performant} algorithms. 
As described earlier, a key design goal of \gls{AMT} models is to enable performance-based
optimizations of code dynamically at runtime.
Runtime-based optimizations come with an associated runtime cost, which is what
motiviates the use of \glspl{task} (rather than, e.g., instructions) as the basis for dynamic runtime transformations.
Existing \gls{AMT} models provide a variety of \glspl{API} for capturing and expressing
\glspl{data-flow dependency} and communicating these to the underlying
\gls{runtime system}.  
One of the adoption challenges many of these \glspl{runtime system} face is
that they require a significant shift away from what has become
the defacto standard of distributed HPC programming: \gls{CSP}. 

\gls{DARMA}'s \gls{programming model} seeks to facilitate the expression of
deferred, asysnchronous work, enabling a \gls{back end} \gls{runtime system}
to perform dynamic runtime optimizations, while making it as simple as possible
for programmers to reason about the correctness of their code.
This motivates \gls{DARMA}'s combined use of successful \gls{programming model}
concepts from a variety of existing \glspl{runtime system}. 
One of \gls{DARMA}'s \gls{programming model} key design decisions is rooted in
the following observations: 
1) all application
developers can effectively reason about how to write correct sequential codes,
2) all MPI
programmers can effectively reason about how to write correct \gls{CSP} codes,
and
3) most applications written in or ported to \gls{DARMA} will likely
have \gls{spmd} as their dominant parallelism.
To simplify the implementation of \gls{spmd}-structured codes, the notion of a
\codelink{rank} is maintained within the \gls{API}.   
By maintaining the notion of a \codelink{rank}, \gls{DARMA} provides application developers a convenience mechanism for creating
the initial problem decomposition and distribution.
Immediately after launch, any user-specified \gls{deferred work} is free to be
migrated by the \gls{runtime system}, if it will result in
better performance.
Because \gls{DARMA} maintains the notion of a \codelink{rank}, it is also
possible for \gls{DARMA} to maintain \gls{CSP}-like semantics (in particular, within an initial
implementation or port of a code prior to the introduction of \gls{deferred
work}). 
This grants developers the ability to express correctness constraints through a familiar and
intuitive \gls{programming model}.  
\gls{DARMA} facilitates the expression of hybrid parallelism by supporting  
\gls{sequential semantics}, within a \codelink{rank}.  This means that application
developers can reason about code as
though it were being deployed sequentially within the \codelink{rank}, even in
the presence of user-specified \gls{deferred work}.   

\gls{DARMA} employs \CC-embedded task annotations for the specification of \gls{deferred
work}. Each block of \gls{deferred work} can be considered a task (coarse-grained blocks of 
\gls{procedural} \gls{imperative} code),  which is not necessarily performed in program order.   Instead,
\gls{deferred work} is performed asynchronously when all of its \glspl{data-flow dependency} are
satisfied.  \Gls{task parallelism} is primarily achieved through permissions/access qualifiers
on data that enable a runtime to reason about which tasks can run in parallel and which tasks are strictly ordered.
Task granularity is determined by the user and annotations are translated by \gls{DARMA}'s \gls{translation
layer} through standard \CC\ constructs (e.g., \glspl{lambda}, \glspl{reference counted pointer}) and \gls{template metaprogramming} to
expose \gls{task parallelism} inherent in the code.
%The issue of granularity is therefore of paramount importance in task-based model.
%How should the extra flexibility of fine-grained tasks be balanced against amortizing the cost of runtime analysis?
%For the time-being, the choice of a correct task granularity seems beyond the abilities of current compilers.
%The application-level API therefore expresses both correctness and granularity.
We note here that \gls{DARMA}'s runtime optimizations are complementary to
compile-time optimizations performed by \emph{performance
portability} tools, e.g.~\cite{Kokkos,RAJA,TiDA}.  Compile-time performance
portability tools provide the ability to map a
single code onto high-performance execution across diverse compute platforms.


One of the major differences between \gls{DARMA} and a traditional \gls{CSP}
\gls{programming model} is the manner in which communication is performed.  
Communication between \gls{DARMA} \codelink{rank}s is not performed via direct messaging.
Instead, \gls{coordination semantics} are used.  Processes \emph{coordinate} by putting/getting
data associated with a unique \codelink{key} in a \gls{tuple space}.  
\Gls{coordination semantics} enables out-of-order message arrival, deferred
execution, task migration, and resilience strategies 
since the application declares or describes the data it needs/produces rather than enforcing an explicit delivery mechanism.
The coordination semantics in the specification are intended to support the use of \gls{zero-copy} mechanisms and tuple caching,
generally producing execution equivalent to an MPI \inlinecode{send/recv} code. 

Together, these features make \gls{DARMA} a mixed \gls{imperative}/\gls{declarative} \gls{programming model}.
As much as possible, sequential \gls{imperative} semantics are used to produce intuitive, maintainable code.
However, the ``\gls{procedural} \gls{imperative}'' function calls and code
blocks do not necessarily execute immediately.
Rather than explicitly perform all work in program order and block on data requests,
they wait for all \glspl{data-flow dependency} to be satisfied.
Such \gls{deferred execution} makes \gls{DARMA} \emph{declarative}, leaving the
exact control-flow up to the \gls{runtime system}.
Furthermore, it is this ability to defer work and advance ahead that gives the \gls{back end}
\gls{runtime  system} the ability to make performance-improving transformations.



%The application-level API therefore expresses both correctness and granularity.
%The issue of task granularity is therefore of paramount importance in a task-based model.
%How should the extra flexibility of fine-grained tasks be balanced against amortizing the cost of runtime analysis?
%For the time-being, the choice of a correct task granularity is beyond the abilities of current compilers.
%As such, choosing task granularity is the responsibility of the application developer.



%Correctness and performance portability must both balance the level of abstraction.
%Algorithms written at too low a level are highly error-prone and not portable.
%In particular, they may over-express correctness constraints.
%A compiler/runtime must obey the correctness requirements (e.g. instruction order) prescribed by an application code.
%Low-level code may be unnecessarily prevent the compiler or runtime from optimizing program execution.
%For example, code hard-wired for a particular loop structure and data layout may perform very well on one system,
%but that may prevent the compiler from tiling optimizations of the loops on other systems. 
%What is the correct way and level of abstraction for programmers to 1) express constraints (correctness) while 2) still enabling the compiler/runtime to transform execution (performance portability) while also 3) allowing developers to direct (hand-optimize) execution when the compiler/runtime misses optimizations apparent to a human developer. 


Although not yet supported in version \specVersion\ of the specification, several
important features will play a role in the \gls{DARMA} \gls{programming model}:

\begin{compactdesc}
\item{\bf Expressive Underlying Abstract Machine Model:}
Notions of \glspl{execution space} and \glspl{memory space} will be introduced
formally in later
versions of the specification.  These abstractions (or similar ones) appear in other runtime
solutions, e.g. \cite{Kokkos, RAJA}. 
   Using such abstractions 1) facilitates performance portable application development across 
  a variety of \glspl{execution space}, and 2)
  provides finer-grained control and additional flexibility in the
  communication of policies regarding data locality and data movement. 
\item{\bf Runtime performance introspection}
  In future versions of the specification \gls{DARMA} will specify hooks for the
  application developer to express, guide, and leverage the use of runtime-level
  performance \gls{introspection}. An important \gls{co-design} activity will include determining
  whether performance \gls{introspection} needs to factor into the
  application-level \gls{programming
  model} on the \gls{front end} or whether it belongs only in the \gls{back end} 
  \gls{runtime system} \gls{API}.
\item{\bf Expression of fine-grained deferred parallel patterns}.
  In future versions of the specification, \gls{DARMA} will 
  specify deferred fine-grained parallel patterns, e.g., deferred
  \inlinecode{parallel-for}, \inlinecode{parallel-scan}, etc.
\item{\bf Instantiating tasks in class member functions}
 Due to idiosyncracies in C++ lambda capture, inline \cwork calls cannot operate on member variables
 within a class member function. 
 Mechanisms for circumventing this C++ limitation will be introduced in later versions.
\item{\bf Subsetting/slicing handles}
 Certain tasks may only access a subset of the data owned by a handle created with \inlinecode{initial_access}.
 Using the handle in a task therefore overexpresses contraints, contrary to the philosophy of DARMA for avoiding unnecessary synchronizations and task preconditions.
 Expressing subsets of classes/slices of arrays will be an important part of future specifications.
\item {\bf Data Staging:}
The memory and execution space concepts introduced above enable 1) performance portable tasks that can run in 
multiple environments through a single code and 2) user-directed (or runtime-directed) asynchronous data movement to move data to compute devices.
\item {\bf Collectives:}
  Some collectives will be supported by \gls{DARMA} in version 0.3.1 of
  the specification, including \inlinecode{all-reduce},
  \inlinecode{reduce-scatter}, and \inlinecode{barrier} collectives.
Collectives will be data-centric rather rank-centric, as done in MPI.
\item{\bf Programmer-directed optimization}
While an abstract algorithm may make more information available to the compiler or runtime for performance-tuning transformations,
compilers and runtime schedulers may not always understand the global nature of the problem.
As such, they may not make performance-improving optimizations that are apparent to an application developer.
A critically important part of future co-design activities will be the
development of the interface by which developers can steer the runtime towards
a desired set of optimizations that compilers or runtime schedulers might fail
to perform.
\end{compactdesc}






 

