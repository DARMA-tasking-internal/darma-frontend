%!TEX root = sandReportSpec.tex
\chapter{Introduction}
\label{chap:introduction}
As we look ahead to next generation platforms and exascale computing,  hardware 
will be characterized by dynamic behavior, increased
heterogeneity, and a marked increase in overall system \gls{concurrency}~\cite{doe_arch, dav_exascale}. 
\gls{AMT} \glspl{programming model} and \glspl{runtime system} 
aim to mitigate these challenges associated with the shifts in \gls{HPC} system architectures.  
\gls{AMT} models strive to exploit all available \gls{task parallelism} and
\gls{pipeline parallelism}, rather than rely solely on \gls{data parallelism}
for \gls{concurrency}. The term {\em \gls{asynchronous}} encompasses the idea that 
1) processes (threads) can diverge to different tasks, rather than execute 
the same tasks in the same order; and 2) \gls{concurrency} is maximized (the 
  minimal amount of synchronization is performed) by 
leveraging multiple forms of parallelism. The term {\em many-task} encompasses 
the idea that the application is decomposed into many 
\gls{migratable} units of work, to enable the overlap of communication and 
computation as well as asynchronous load balancing strategies.

The \gls{AMT} community is currently very active (e.g.,
~\cite{OCR,STAPL,Legion,Realm,StencilHPX,Charm++,Uintah,Loci,PARSEC,DaGuE,Cilk}),
representing a range of different design points within the
design space of \gls{AMT} models. The community has not yet standardized, and
a variety of options are still being explored with regards to \gls{programming model},
\gls{execution model}, \gls{memory model}, and \gls{data model}.  Across all of
these model axes, individual design decisions lend themselved to a more
\gls{declarative} (placing more resposnisbility on the \gls{runtime system}) vs.  
\gls{imperative} (placing more responsibility on the application developer)
software stacks. 

\begin{compactdesc}
\item[Programming model:]
From a \gls{programming model} perspective,
\gls{AMT} models all have some notion of decomposing applications into small,
\gls{migratable} units of work. 
\Gls{task parallelism} can be expressed in an \gls{imperative}
fashion, with users managing the \glspl{data-flow dependency} explicitly
themselves.  Conversely, \gls{task parallelism} can be expressed in a \gls{declarative}
fashion, where \glspl{data-flow dependency} are managed within the associated \gls{runtime system}. 
  In \gls{declarative} \gls{AMT} \glspl{programming model}, users must provide
  the \gls{runtime system} with sufficient information to manage
  \glspl{data-flow dependency}: this includes information regarding the
  structure of the data and permissions/access requirements (e.g. \gls{RAW}/
  \gls{WAR}).  In general, \gls{declarative} models are best suited for coarse-grained \gls{task
parallelism}, as \gls{runtime system} overheads increase with the mangement of
\glspl{data-flow dependency}.  
\item[Execution model:]
  \gls{AMT} \glspl{execution model} can be viewed as the coarse-grained,
  distributed memory analog of instruction-level parallelism. 
  \gls{AMT} \glspl{runtime system} extend the concepts of \gls{data
  prefetching}, out-of-order task execution based on dependency analysis (either performed
    imperatively by the user, or deduced declaratively by the \gls{runtime
    system}).     \Glspl{execution model} specify in part how \gls{concurrency} is
    managed. For example, \gls{AMT} runtimes implement a variety of
    \glspl{execution model}, including \gls{event-based}, \gls{fork-join} (either
    \gls{fully strict} or \gls{terminally strict}), and the \gls{actor
    model}. These different \glspl{execution model} can be categorized by four broad
    categories: \gls{conservative execution}, \gls{phased execution}, 
    \gls{copy-on-write data-flow execution}, and \gls{speculative execution}.
    On one side of the spectrum lies \gls{conservative execution}, where the \gls{runtime system} only spawns tasks in parallel 
    that are guaranteed not to conflict.  The application exposes \gls{RAW}/\gls{WAR} conflicts, allowing the
    \gls{runtime system} to decide which tasks can safely run in parallel.  Independent threads do not need to explicitly synchronize; 
    rather, execution begins with zero \gls{concurrency} and grows conservatively to the maximum allowed \gls{concurrency}.
    Often this means that programs begin with a single top-level task that forks new tasks 
    to increase \gls{concurrency}.  On the other side of the spectrum lies
    \gls{phased execution}, where many parallel workers begin work simultaneously, (naturally supporting a \gls{spmd}
    style of programming) and \glspl{phase barrier} guarantee safe execution. 
    \Gls{conservative execution} is a more \gls{declarative} form of managing
    initial problem distribution; the \gls{spmd} launching of work in \gls{phased execution} 
    is much more \gls{imperative}, as it is prescribed by the application
    developer.
%     While \gls{AMT} models also naturally lend themselves to 
%    \gls{speculative execution}, we are not aware
%    of this feature being supported currently within any of the leading \gls{AMT} runtimes.
\item[Memory model:]
  \gls{HPC} \glspl{memory model} are either truly \gls{distributed memory
  model} or some variant of a \gls{distributed shared memory model} (e.g., a \gls{PGAS}-like model).  In a
     \gls{distributed memory model} each processor has its own private memory. 
     Computational tasks can only operate on their local data. When remote data
     is required, it is communicated between the remote and local tasks.
     In a \gls{distributed shared memory model}, physically distinct memory can be accessed as one
    logically shared address space.  In both of these models, memory can be
    directly accessed directly via 1) its address, or 2) indirectly via
    \gls{coordination semantics}. In the second scenario, parallel workers never directly
    communicate, rather they ``coordinate'' indirectly via a \gls{key-value
    store} or \gls{tuple space}.  \Glspl{distributed shared memory model} are
    often considered a more \gls{declarative} programming style, due to the
    single logical address space.  However both \glspl{memory model} support
    a \gls{declarative} style of programming when using \gls{coordination
    semantics}.
\item[Data model:]
 \Gls{AMT} models that support a more \gls{declarative} style of programming,
  especially with regards to \gls{data-flow dependency} management, 
  can make much more effective data management decisions, e.g., \gls{slicing} the data to minimize hazards, performing \glspl{interference test} for dependency
  analysis, and performing \gls{serialization} to stage data on remote resources when needed.  
  One option for providing structural information regarding data is to impose a
  \gls{data model}.  Another option is to require application developers to define
  \gls{serialization}, \gls{slicing}, and \glspl{interference test} for their data blocks. 
\end{compactdesc}


\section{Scope}\label{sec:scope}
In this document we provide the specification for \gls{DARMA},
an application-driven \gls{programming model} and \gls{runtime system} co-design 
research vehicle.
\gls{DARMA} comprises three layers: the \gls{front end} \gls{API},
\gls{translation layer}, and
\gls{back end} \gls{API}. 
The \gls{front end} is the user-level \gls{API}. 
Its use has the feel of an \gls{EDSL} in \CC,  inheriting the generic
language constructs of \CC and adding \gls{semantics} that facilitate
distributed parallel programming.  Though the \gls{front end} \gls{EDSL} uses
\CC constructs in non-traditional ways to implement these semantics, it is
nonetheless fully embedded in the \CC language and 
requires a widely supported subset of \CC{}14 functionality \compilerReqs.
The \gls{translation layer} leverages \CC\ \gls{template
metaprogramming} to map the user's code onto the \gls{back end} \gls{API}.
The \gls{back end} \gls{API} is a set of abstract classes and function
signatures that \gls{runtime system} developers must implement in accordance with the
specification requirements in order to interface with application code written
to the \gls{DARMA} \gls{front end}.  


\gls{DARMA} aims to serve four primary functions:
\begin{compactdesc}
\item[Insulate applications from runtime system and hardware idiosyncrasies:]
  \gls{AMT} models are, by their very definition, a more \gls{declarative} style of
  programming than currently deployed \gls{CSP} model. 
  While this provides application developers insulation from hardware
  idiosyncrasies, the \gls{AMT} community currently lacks best practices and standards.
  \gls{DARMA}'s design includes separate application-facing \gls{front end} and
  \gls{runtime system}-facing \gls{back end} \glspl{API}. This separation of concerns 
  enables an application team to explore the impact of \gls{runtime system}
  design space decisions. For example, application developers can build their code using different
  \gls{DARMA}-compliant \gls{back end} implementations, without
  having to deal with the combinatorial complexity of
  implementing their application in many different \gls{front end} \glspl{API}. 
  It should be noted that \gls{DARMA}'s \gls{front end} \gls{API} is not
  intended to be static  -- it will evolve based on \gls{co-design} feedback from both application
  and \gls{runtime system} developers. 
\item[Improve AMT runtime programmability by co-designing a front end  API directly with application developers:]
  Recent work~\cite{L2Sand2015} highlighted gaps with respect to productivity
  in some existing \gls{AMT} \glspl{runtime system}, in particular noting requirements gaps and 
  deficiencies in existing \glspl{API}. Co-designing \gls{DARMA}'s \gls{front end} \gls{API}
  directly with application developers provides a mechanism for capturing
  different application's \gls{runtime system} requirements-- giving them a voice in the design of an
  asynchronous tasking \gls{API}.  Experimenting with the \gls{API} provides an agile method for application
  developers to reason about the \gls{API} 
  %(e.g., does it allow them to intuitively express their algorithms?) 
  and better articulate their \gls{runtime system} execution requirements.
\item[Synthesize application co-design activities into meaningful requirements
  for runtimes:]
  The specification provides a mechanism for tracking the
  provenance of design decisions and requirements as they evolve throughout the
  \gls{co-design} process. Chapter~\ref{chap:requirements} provides a list of the application
  requirements gathered,  and Chapter~\ref{chap:evolution} tracks the evolution of
  the specification, highlighting which requirements motivated changes to the specification.
  \Gls{runtime system} software stack developers benefit 
  from 1) \gls{DARMA}’s application-informed requirements, and 2) access to code
  kernels and proxy applications developed via the \gls{front end}
  \gls{co-design} process.
\item[Facilitate AMT design space characterization, accelerating the
development of AMT best practices:]
  In the discussion above we summarize a range of high-level design decisions for
  \gls{AMT} programming, execution, memory, and data models. \gls{DARMA}'s
  separation of \gls{front end} and \gls{back end} \glspl{API} seeks to
  facilitate this design space characterization and exploration.  There 
  is a notable tension between the design of 1) a \gls{front end} \gls{API} that is expressive, simple, 
  and easy to incorporate within existing application code bases, and 2) a
  \gls{back end} \gls{API} comprising sufficiently multi-paradigm abstractions 
  to support multiple \gls{DARMA}-compliant implementations that leverage existing \gls{runtime
  system} technologies. 
  Consequently, \gls{DARMA} \gls{API}s (both \gls{front end} and \gls{back end}) are
  intended to evolve based on feedback from the \gls{co-design} cycles with 
  application, \gls{programming model}, and \gls{runtime system} teams. 
\end{compactdesc}


This document captures version \specVersion\ of the \gls{DARMA} specification. 
Sections~\ref{sec:programmingmodel}--\ref{sec:datamodel} provide a brief
summary regarding the compatible programming, execution, memory, and data
models supported within \gls{DARMA}. 
All design decisions, first and foremost, are made to best support application
requirements.  When possible, we have aimed to  
specify multi-paradigm abstractions and make flexible design decisions that
facilitate \gls{back end} implementation leveraging existing \gls{AMT}
technologies.  
%However, some \gls{DARMA} design decisions may preclude the use of a subset of existing \gls{AMT} 
%technologies, without significant modifications.
Lastly, we note that the features detailed in Chapters~\ref{chap:front_end} and
~\ref{chap:back_end}  are not entirely comprehensive -- meaning they do not yet capture all of the
application requirements driving \gls{DARMA} \gls{co-design}.  This is because 
we are formalizing the specification process from the inception of \gls{DARMA}, layering-in features incrementally to
provide the community an opportunity for input, and active engagement in the
\gls{co-design} process.  Suggested enhancements and changes 
to the \gls{DARMA} specification are welcome and can be made via a \gls{DEP} (see
Appendix~\ref{chap:DEP} for details on this process and a \gls{DEP} template).



\input{programming_model}
\input{execution_model}
\input{memory_model}
\input{data_model}



\section{Document organization}
\label{sec:organization}
This docuemnt is organized as follows.  In Chapter~\ref{chap:front_end} we
introduce the \gls{front end} \gls{API}.  In
Chapter~\ref{chap:translation_layer} we
provide a description of the \gls{translation layer}, and in
Chapter~\ref{chap:back_end} we provide the specifics regarding what must be
supproted by each of the \gls{back end} abstract classes in order to implement
the DARMA specification. In Chapter~\ref{chap:requirements} we include a list
of application requirements driving the specification (along with a list of the
    applications contributing to the requirements to date).
We conclude this document with
Chapter~\ref{chap:evolution}, which includes a brief history of changes between
previous versions of the specificaiton, along with a list of the planned changes 
in upcoming versions.
Appendix~\ref{chap:examples} provides a suite of examples that illustrate the
key \gls{front end} \gls{API} features. Appendix~\ref{chap:vasp} provides
additional technical details regarding %\gls{vasps}.
Appendix~\ref{chap:DEP} provides information regarding how the broader
community can shape the \gls{DARMA} specification and includes a template for 
a \gls{DEP}. 

