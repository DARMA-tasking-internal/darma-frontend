%!TEX root = sandReportSpec.tex
\chapter{Introduction}
\label{chap:introduction}
As we look ahead to next generation platforms and exascale computing,  hardware 
will be characterized by dynamic behavior, increased
heterogeneity, decreased reliability, and a marked increase in system
\gls{concurrency}, both on-node and system-wide~\cite{doe_arch, dav_exascale}. 
\gls{AMT} \glspl{programming model} and \glspl{runtime system} 
aim to mitigate the challenges associated with the shifts in \gls{HPC} system architectures.  
\gls{AMT} models strive to exploit all available \gls{task parallelism} and
\gls{pipeline parallelism}, rather than rely solely on \gls{data parallelism}
for \gls{concurrency}. The term {\em \gls{asynchronous}} encompasses the idea that 
1) processes (threads) can diverge to different tasks, rather than execute 
the same tasks in the same order; and 2) \gls{concurrency} is maximized (the 
  minimal amount of synchronization is performed) by 
leveraging multiple forms of parallelism. The term {\em many-task} encompasses 
the idea that the application is decomposed into many 
\gls{migratable} units of work, to enable the overlap of communication and 
computation as well as asynchronous load balancing strategies.

The \gls{AMT} community is currently very active (e.g.,
~\cite{OCR,STAPL,Legion,Realm,StencilHPX,Charm++,Uintah,Loci,PARSEC,DaGuE,Cilk}),
representing a range of different design points within the
design space of \gls{AMT} models. While the technologies show significant
potential to address challenges, the community has not yet identified best
practices and existing systems still represent \gls{programming model},
\gls{execution model}, \gls{memory model}, and \gls{data model}.  

\begin{compactdesc}
\item[Programming model:]
From a \gls{programming model} perspective,
\gls{AMT} models all have some notion of decomposing applications into small,
\gls{migratable} units of work. 
\Gls{task parallelism} can be expressed in a \gls{fork-join}
fashion, with users managing the \glspl{data-flow dependency} explicitly
themselves.  Conversely, \glspl{data-flow dependency} can be managed explicitly within the associated \gls{runtime system}. 
  In \gls{declarative} \gls{AMT} \glspl{programming model}, users must provide
  the \gls{runtime system} with sufficient information to manage
  \glspl{data-flow dependency}: this includes information regarding the
  structure of the data and permissions/access requirements (e.g. \gls{RAW}/
  \gls{WAR}).  In general, data-flow models are best suited for coarse-grained \gls{task
parallelism}, as \gls{runtime system} overheads increase with the mangement of
\glspl{data-flow dependency}.  
\item[Execution model:]
  \gls{AMT} \glspl{execution model} can be viewed as the coarse-grained,
  distributed memory analog of instruction-level parallelism. 
  \gls{AMT} \glspl{runtime system} extend the concepts of \gls{data
  prefetching}, out-of-order task execution based on dependency analysis (either performed
    imperatively by the user, or deduced declaratively by the \gls{runtime
    system}).     \Glspl{execution model} specify in part how \gls{concurrency} is
    managed. For example, \gls{AMT} runtimes implement a variety of
    \glspl{execution model}, including \gls{event-based}, \gls{fork-join} (either
    \gls{fully strict} or \gls{terminally strict}), and the \gls{actor
    model}. These different \glspl{execution model} can be categorized by four broad
    categories: \gls{conservative execution}, \gls{phased execution}, 
    \gls{copy-on-write data-flow execution}, and \gls{speculative execution}.
    On one side of the spectrum lies \gls{conservative execution}, where the \gls{runtime system} only spawns tasks in parallel 
    that are guaranteed not to conflict.  The application exposes \gls{RAW}/\gls{WAR} conflicts, allowing the
    \gls{runtime system} to decide which tasks can safely run in parallel.  Independent threads do not need to explicitly synchronize; 
    rather, execution begins with zero \gls{concurrency} and grows conservatively to the maximum allowed \gls{concurrency}.
    Often this means that programs begin with a single top-level task that forks new tasks 
    to increase \gls{concurrency}.  On the other side of the spectrum lies
    \gls{phased execution}, where many parallel workers begin work simultaneously, (naturally supporting a \gls{spmd}
    style of programming) and \glspl{phase barrier} guarantee safe execution. 
    \todo[inline]{Jeremy: These terms were from the RESPA paper.  Do you want
    to refine/change the messaging here? Or is it ok as is?}
%     While \gls{AMT} models also naturally lend themselves to 
%    \gls{speculative execution}, we are not aware
%    of this feature being supported currently within any of the leading \gls{AMT} runtimes.
\item[Memory model:]
  \gls{HPC} \glspl{memory model} are either truly \gls{distributed memory
  model} or some variant of a \gls{distributed shared memory model} (e.g., a \gls{PGAS}-like model).  In a
     \gls{distributed memory model} each processor has its own private memory. 
     Computational tasks can only operate on their local data. When remote data
     is required, it is communicated between the remote and local tasks.
     In a \gls{distributed shared memory model}, physically distinct memory can be accessed as one
     logically shared address space.  \gls{PGAS} models fall in between, with a
     logically global address space, with a portion remaining local/private to
     a particular process.  Across the spectrum of memory models, memory can be
    directly accessed directly via 1) its address, or 2) indirectly via a
    \gls{coordination semantics}. In the second scenario, parallel workers never directly
    communicate, rather they ``coordinate'' indirectly via a \gls{key-value
    store} or \gls{tuple space}.  
\item[Data model:]
  In order for data-flow \gls{AMT} models to make effective data management
  decisions (e.g., \gls{slicing} the data and making copies to increase
  parallelism  by minimizizing hazards), they must have some knowledge of the
  structure of the data.
  One option for providing structural information regarding data is to impose a
  \gls{data model}.  Another option is to require application developers to define
  \gls{serialization}, \gls{slicing}, and \glspl{interference test} for their data blocks. 
\end{compactdesc}

\section{Scope}\label{sec:scope}
Although the \gls{AMT} model community is quite active, the lack of
standards impedes adoption of these solutions by the application community.
Although it is premature to standardize, we believe there is sufficient  
breadth and depth in the research community to begin developing community best
practices.  Towards this end, this document proivides the specification for \gls{DARMA},
a research vehicle for \gls{AMT} \gls{programming model} \gls{co-design}.  
\gls{DARMA} aims to serve four primary purposes:
\begin{compactdesc}
\item[Insulate applications from runtime system and hardware idiosyncrasies:]
  \gls{AMT} models are, by their very definition, a more \gls{declarative} style of
  programming than the currently deployed \gls{CSP} model. 
  \gls{DARMA}'s design includes separate application-facing \gls{front end} and
  \gls{runtime system}-facing \gls{back end} \glspl{API}. This separation of concerns 
  enables an application team to explore the impact of \gls{runtime system}
  design space decisions. For example, application developers can build their code using different
  \gls{DARMA}-compliant \gls{back end} implementations, without
  having to deal with the combinatorial complexity of
  implementing their application in many different \gls{front end} \glspl{API}. 
  It should be noted that \gls{DARMA}'s \gls{front end} \gls{API} is not
  fixed -- it will evolve based on \gls{co-design} feedback from both application
  and \gls{runtime system} developers. 
\item[Improve AMT runtime programmability by co-designing a front end  API directly with application developers:]
  Recent work~\cite{L2Sand2015} highlighted gaps with respect to productivity
  in some existing \gls{AMT} \glspl{runtime system}, in particular noting requirements gaps and 
  deficiencies in existing \glspl{API}. Co-designing \gls{DARMA}'s \gls{front end} \gls{API}
  directly with application developers provides a mechanism for capturing
  different application's \gls{runtime system} requirements-- giving them a voice in the design of an
  asynchronous tasking \gls{API}.  Experimenting with the \gls{API} provides an agile method for application
  developers to reason about the \gls{API} 
  %(e.g., does it allow them to intuitively express their algorithms?) 
  and better articulate their \gls{runtime system} execution requirements.
\item[Synthesize application co-design activities into meaningful requirements
  for runtimes:]
  The specification provides a mechanism for tracking the
  provenance of design decisions and requirements as they evolve throughout the
  \gls{co-design} process. Chapter~\ref{chap:requirements} provides a list of the application
  requirements gathered,  and Chapter~\ref{chap:evolution} tracks the evolution of
  the specification, highlighting which requirements motivated changes to the specification.
  \Gls{runtime system} software stack developers benefit 
  from 1) \gls{DARMA}â€™s application-informed requirements, and 2) access to code
  kernels and proxy applications developed via the \gls{front end}
  \gls{co-design} process.
\item[Facilitate AMT design space characterization, accelerating the
development of AMT best practices:]
  In the discussion above we summarize a range of high-level design decisions for
  \gls{AMT} programming, execution, memory, and data models. \gls{DARMA}'s
  separation of \gls{front end} and \gls{back end} \glspl{API} seeks to
  facilitate this design space characterization and exploration.  There 
  is a notable tension between the design of 1) a \gls{front end} \gls{API} that is expressive, simple, 
  and easy to incorporate within existing application code bases, and 2) a
  \gls{back end} \gls{API} comprising sufficiently multi-paradigm abstractions 
  to support multiple \gls{DARMA}-compliant implementations that leverage existing \gls{runtime
  system} technologies. 
  Consequently, \gls{DARMA} \gls{API}s (both \gls{front end} and \gls{back end}) are
  intended to evolve based on feedback from the \gls{co-design} cycles with 
  application, \gls{programming model}, and \gls{runtime system} teams. 
\end{compactdesc}


The rest of this chapter provides a high-level description of 
\gls{DARMA}'s structural design  along with a brief summary of \gls{DARMA}'s programming,
memory, data, and (compatible) execution models. We note that throughout the
\gls{co-design} process, decisions are first and foremost, 
made to best support application requirements.  Furthermore, we target a
\gls{back end} representation that supports \gls{AMT} \gls{runtime system}
design space exploration, by facilitating build out of \gls{DARMA}-compliant
back ends with a variety of existing \gls{AMT} \gls{runtime system}
technologies.
Lastly, we note that the features detailed in Chapters~\ref{chap:front_end} and
~\ref{chap:back_end}  are not entirely comprehensive -- meaning they do not yet capture all of the
application requirements driving \gls{DARMA} \gls{co-design}.  This is because 
we are formalizing the specification process from the inception of \gls{DARMA}, layering-in features incrementally to
provide the community opportunity for input, and active engagement in the
\gls{co-design} process.  Suggested enhancements and changes 
to the \gls{DARMA} specification are welcome and can be made via a \gls{DEP} (see
Appendix~\ref{chap:DEP} for details on this process and a \gls{DEP} template). 

\subsection{High-level Design}
\gls{DARMA} is a translation layer between an 
application-facing \gls{front end} \gls{API} and a \gls{runtime system} facing \gls{back end} 
\gls{API}. 

\paragraph{Application-facing front end API:}
The \gls{front end} \gls{API} is the center of \gls{programming model}
activities.  \gls{DARMA}'s \gls{front end} \gls{API} 
is an \gls{EDSL} in \CC,  inheriting the generic
language constructs of \CC and adding \gls{semantics} that facilitate
distributed, deferred, asynchronous, parallel programming. Though the \gls{EDSL} uses
\CC constructs unfamiliar to many programmers to implement these semantics, 
it is nonetheless fully embedded in the \CC language and
requires a widely supported subset of \CC{}14 functionality \compilerReqs.

\paragraph{Translation layer:}
For context, we note here that code transformations are already ubiquitous at the compiler-level.
Compilers will add, delete, swap, or reorder instructions to avoid unnecessary operations, improve data locality, or improve pipelining.
\gls{DARMA} aims to enable analogous transformations dynamically at runtime.
Many transformations of program execution that benefit performance will be unknowable until the program actually runs,
however these dynamic runtime optimizations are much more expensive than compile-time optimizations.
Consequently, \emph{tasks} are the basis of \gls{DARMA} dynamic transformations
(in an analogous way that instructions are the basis of static compiler
transformations).

\gls{DARMA}'s \gls{translation layer} leverages \CC\ \gls{template
metaprogramming} to map the user's \gls{front end} \gls{API} calls onto the \gls{back end} runtime \gls{API},
bridging the \gls{programming model} and actual program execution.
We note however that the \gls{DARMA} \gls{translation layer} itself does not
perform any optimizations -- these are left entirely to the \gls{back end} \gls{runtime system} implementations.
Rather, the translation layer converts the application code specified with
\gls{DARMA}'s \gls{front end} \gls{API} into
an ``intermediate representation'' that enables a \gls{runtime system} to make
intelligent, dynamic decisions (e.g., 
about task order and task locality or possibly even task deletion and task
replication when appropriate).




\paragraph{Runtime-facing back end API:}
The \gls{back end} \gls{API} is a set of abstract classes and function
signatures that \gls{runtime system} developers must implement in accordance with the
specification requirements in order to interface with application code written
to the \gls{DARMA} front end. 
Strictly speaking, the \gls{back end} \gls{API} calls only generate a stream of
\emph{deferred tasks} (tasks with corresponding data inputs/outputs) that
implicitly capture the program's data flow.

However, we note the information passed through the \gls{translation layer} to
the \gls{back end} is sufficient
to (and intended to) support a \gls{CDAG} representation of the application.
In a \gls{task-DAG} representation, tasks are vertices $V$ in a graph $G$ with directed edges $E$.
An edge from vertex $v_1$ to vertex $v_2$ indicates a precedence constraint.
A \gls{CDAG} representation describes task-data precedence constraints, rather
than just task-task precedence constraints.  In a \gls{CDAG} there are two
types of vertices - tasks $T$ and data $D$ that compose the complete set of vertices $V$.
Edges never directly connect two tasks and instead edges are only ever described between a task vertex, $t$, and a data vertex, $d$
indicating that (depending on direction of the edge) data is either consumed or produced by a task.
The \gls{task-DAG} indicating task-task precedence constraints can always be
obtained form the \gls{CDAG}, which captures the data-flow task graph.
The \gls{CDAG} is thus more general, capturing additional information to
enable runtime code transformations.
Although beyond the scope of this specification document, the interested reader
will find numerous works discussing heuristics and order-preserving convex
transformations of task graphs that demonstrate the utility of a coarse-grained
\gls{CDAG} for enabling dynamic runtime optimization of an algorithm~\cite{}.
We reiterate, though, that the \gls{CDAG} is only a concept guiding the design
of the \gls{back end} \gls{API} and not strictly part of the \gls{DARMA} specificaion.


Finally, we highlight that a \gls{DARMA} executable application must link to a \gls{runtime
  system} that implements the abstract \gls{back end} runtime \gls{API}.
It is intended that these implementations will be external, drawing upon
existing AMT technologies.  However,  a reference implementation will be provided
in the \gls{DARMA} code distribution.



\input{programming_model}
\input{execution_model}
\input{memory_model}
\input{data_model}



\section{Document organization}
\label{sec:organization}
This docuemnt is organized as follows.  In Chapter~\ref{chap:front_end} we
introduce the \gls{front end} \gls{API}.  In
Chapter~\ref{chap:translation_layer} we
provide a description of the \gls{translation layer}, and in
Chapter~\ref{chap:back_end} we provide the specifics regarding what must be
supproted by each of the \gls{back end} abstract classes in order to implement
the DARMA specification. In Chapter~\ref{chap:requirements} we include a list
of application requirements driving the specification (along with a list of the
    applications contributing to the requirements to date).
We conclude this document with
Chapter~\ref{chap:evolution}, which includes a brief history of changes between
previous versions of the specificaiton, along with a list of the planned changes 
in upcoming versions.
Appendix~\ref{chap:examples} provides a suite of examples that illustrate the
 \gls{front end} \gls{API} features. Appendix~\ref{chap:vasp} provides
additional technical details regarding %\gls{vasps}.
Appendix~\ref{chap:DEP} provides information regarding how the broader
community can shape the \gls{DARMA} specification and includes a template for 
a \gls{DEP}. 

